{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pntHKXcR2Ib3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269ddb60-73ed-4cce-f293-c170f826eab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ta) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=243910ca956d41a178cdfac508755e857551cd7e8711a68b62de5642adfd6e46\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/67/4f/8a9f252836e053e532c6587a3230bc72a4deb16b03a829610b\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ta\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import ta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('appl_day_data0.csv')"
      ],
      "metadata": {
        "id": "xDMYLSgz2emz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = data.shape[0]"
      ],
      "metadata": {
        "id": "agQV0KwH25Wx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Past prediction is not an indicator for future outcome\n",
        "Data:\n",
        "*   Normalize to Z-Standardization\n",
        "*   Stagger Data?\n",
        "*   Create sequence\n",
        "\n",
        "Inputs:\n",
        "  Volume: higher volume = greater chance of movement\n",
        "  RSI\n",
        "  MA"
      ],
      "metadata": {
        "id": "V_Rzy5C7d9nv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NeuralNet(nn.modele) creates a class called NeuralNet that inherits from nn.Module\n",
        "nn.Module is a base class for neural network modules in PyTorch\n",
        "\n",
        "__init__ initializes the neural network with parameters input_size, hidden_size, and output_size\n",
        "  each parameter defines how many neurons in each layer\n",
        "\n",
        "self.l1: takes input_size number of inputs and produces hidden_size outputs\n",
        "self.sigmoid: activation function that converts the output of self.l1\n",
        " converts the outputs into numbers bt 0 and 1\n",
        "self.l2: takes hidden_size inputs and produces output_size outputs\n",
        "self.relu: another activation function; negative values turn to 0\n",
        "\n",
        "forward: defines how data moves through the network\n",
        " applies first layer, then sigmoid, then second layer, then relu\n",
        " returns the output at the end\n",
        "\n",
        "training:\n",
        "criterion: defines the loss function that measures the difference bt predicted and actual values\n",
        " we use Mean Squared Error (like from regression)\n",
        "optimizer: sets up the Adam optimizer; adjusts the weights during training\n",
        " learning rate (lrn) controls how much the weights are updated\n",
        "training loop:\n",
        " outer loop runs for specified epochs (full passes through dataset)\n",
        " inner loop goes through each example in dataset\n",
        " forward pass: computes output using forward method\n",
        " loss: calculates the loss (from criterion)\n",
        " optimizer.zero_grad(): clears previous gradients\n",
        " loss.backward(): calculates gradients of the loss with respect to model paramters\n",
        " optimizer.step(): updated model parameters based on gradients\n",
        "\n",
        "we calculate loss to see how well the model is performing\n",
        " loss is how far output is from train_dataset\n",
        " small loss is better performance\n",
        "\n",
        "we clear gradients from previous iteration. gradients don't clear up from iteration to it.\n",
        "loss.backward(): uses backpropogation to compute loss with respect to each model parameter\n",
        " allows each neuron weight to be adjusted based on its contribution to the loss\n",
        "optimizer.step(): updated models parameters based on gradients\n",
        " means that we change the weights in a direction that minimizes the loss\n",
        "\n",
        "\n",
        "Things to think about for improvement:\n",
        "\n",
        "Batch Training:\n",
        " Processs several examples simulatensouly to improve training speed and convergence\n",
        "\n",
        "Learning Rate Scheduling\n",
        " Adjust the learning rate during training can help model converge more effectively\n",
        "\n",
        "Regularization:\n",
        " dropout: drop random neurons to promote robustness\n",
        " add penalties to the loss function for larger weights\n",
        "\n",
        "add more layers"
      ],
      "metadata": {
        "id": "OSA8xRI-Um7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Actual Code"
      ],
      "metadata": {
        "id": "ehWIgCr2V9Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "      super(NeuralNet, self).__init__()\n",
        "      self.l1 = nn.Linear(input_size, hidden_size)\n",
        "      self.tan = nn.Tanh()\n",
        "      self.l2 = nn.Linear(hidden_size, output_size)\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x.view(x.size(0), -1)\n",
        "      out = self.l1(x)\n",
        "      out = self.tan(out)\n",
        "      out = self.l2(out)\n",
        "      out = self.softmax(out)\n",
        "      return out\n",
        "\n",
        "    def train_model(self, epochs, lrn, train_loader):\n",
        "      #using Binary Cross Entropy Loss bc it works well with probabilities\n",
        "      #I can also try CrossEntropyLoss that woeks with softmax layer\n",
        "      # remove softmax in model to use\n",
        "      # uses a negative log-likelihood loss which is more numerically stable\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = torch.optim.Adam(self.parameters(), lr=lrn)\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "        #train_loader because we're working in batches\n",
        "        for inputs, targets in train_loader:\n",
        "          #forward\n",
        "          output = self(inputs)\n",
        "          loss = criterion(output, targets)\n",
        "\n",
        "          #backwards\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "        #Print the loss to monitor the progress\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "6-pzzpfaaAtg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA\n",
        "#Preporocess data by scaling it (between 0 and 1 using MinMaxScaler)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#cleaning the data (converting to numbers)\n",
        "data['Close/Last'] = data['Close/Last'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
        "data['Open'] = data['Open'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
        "data['High'] = data['High'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
        "data['Low'] = data['Low'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
        "\n",
        "#10 day rolling average\n",
        "data['MA10'] = data['Close/Last'].rolling(window=10).mean()\n",
        "\n",
        "#RSI\n",
        "data['RSI'] = ta.momentum.RSIIndicator(close=data['Close/Last'], window=14).rsi()\n",
        "\n",
        "#Droping NA values\n",
        "data = data[['Open', 'Close/Last', 'High', 'Low', 'MA10', 'RSI']].dropna()\n",
        "\n",
        "#Scaling data to Z-Standard\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "scaled_data = scaled_data * 2\n",
        "\n",
        "X = scaled_data[:-1]  # All rows except the last one\n",
        "y = (scaled_data[1:, 1] > scaled_data[:-1, 1]).astype(int)  # Compare today's closing with tomorrow's\n",
        "y = np.column_stack([y, 1 - y])  # One-hot encoding: [1, 0] for up, [0, 1] for down\n",
        "\n",
        "# Step to split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to tensors\n",
        "x_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.FloatTensor(y_train)\n",
        "x_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.FloatTensor(y_test)\n",
        "\n",
        "# Create DataLoader for the training set\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=40, shuffle=True)"
      ],
      "metadata": {
        "id": "aMC4iGcmjNBv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Initialize the model\n",
        "input_size = 6  # 12 days * 6 features\n",
        "hidden_size = 6  # You can experiment with this\n",
        "output_size = 2  # Up/Down prediction\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size)\n",
        "\n",
        "# 4. Train the model\n",
        "epochs = 350\n",
        "learning_rate = 0.006\n",
        "\n",
        "model.train_model(epochs, learning_rate, train_loader)"
      ],
      "metadata": {
        "id": "XVSESW4JXCSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "232605e9-8767-4026-81e8-0a5af83444f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/350], Loss: 0.7475\n",
            "Epoch [2/350], Loss: 0.6241\n",
            "Epoch [3/350], Loss: 0.6688\n",
            "Epoch [4/350], Loss: 0.6548\n",
            "Epoch [5/350], Loss: 0.6508\n",
            "Epoch [6/350], Loss: 0.8184\n",
            "Epoch [7/350], Loss: 0.6692\n",
            "Epoch [8/350], Loss: 0.6753\n",
            "Epoch [9/350], Loss: 0.6865\n",
            "Epoch [10/350], Loss: 0.6994\n",
            "Epoch [11/350], Loss: 0.8195\n",
            "Epoch [12/350], Loss: 0.6563\n",
            "Epoch [13/350], Loss: 0.8308\n",
            "Epoch [14/350], Loss: 0.6947\n",
            "Epoch [15/350], Loss: 0.6232\n",
            "Epoch [16/350], Loss: 0.6246\n",
            "Epoch [17/350], Loss: 0.6163\n",
            "Epoch [18/350], Loss: 0.5972\n",
            "Epoch [19/350], Loss: 0.5672\n",
            "Epoch [20/350], Loss: 0.6896\n",
            "Epoch [21/350], Loss: 0.6079\n",
            "Epoch [22/350], Loss: 0.7543\n",
            "Epoch [23/350], Loss: 0.6003\n",
            "Epoch [24/350], Loss: 0.7650\n",
            "Epoch [25/350], Loss: 0.6777\n",
            "Epoch [26/350], Loss: 0.7545\n",
            "Epoch [27/350], Loss: 0.6490\n",
            "Epoch [28/350], Loss: 0.7277\n",
            "Epoch [29/350], Loss: 0.7354\n",
            "Epoch [30/350], Loss: 0.7212\n",
            "Epoch [31/350], Loss: 0.7891\n",
            "Epoch [32/350], Loss: 0.6784\n",
            "Epoch [33/350], Loss: 0.6565\n",
            "Epoch [34/350], Loss: 0.6321\n",
            "Epoch [35/350], Loss: 0.7387\n",
            "Epoch [36/350], Loss: 0.7122\n",
            "Epoch [37/350], Loss: 0.4862\n",
            "Epoch [38/350], Loss: 0.6782\n",
            "Epoch [39/350], Loss: 0.6185\n",
            "Epoch [40/350], Loss: 0.6907\n",
            "Epoch [41/350], Loss: 0.7293\n",
            "Epoch [42/350], Loss: 0.5918\n",
            "Epoch [43/350], Loss: 0.7856\n",
            "Epoch [44/350], Loss: 0.6899\n",
            "Epoch [45/350], Loss: 0.7523\n",
            "Epoch [46/350], Loss: 0.6419\n",
            "Epoch [47/350], Loss: 0.5488\n",
            "Epoch [48/350], Loss: 0.6254\n",
            "Epoch [49/350], Loss: 0.6372\n",
            "Epoch [50/350], Loss: 0.8952\n",
            "Epoch [51/350], Loss: 0.7988\n",
            "Epoch [52/350], Loss: 0.6286\n",
            "Epoch [53/350], Loss: 0.6085\n",
            "Epoch [54/350], Loss: 0.5236\n",
            "Epoch [55/350], Loss: 0.8333\n",
            "Epoch [56/350], Loss: 0.5661\n",
            "Epoch [57/350], Loss: 0.6127\n",
            "Epoch [58/350], Loss: 0.6265\n",
            "Epoch [59/350], Loss: 1.0298\n",
            "Epoch [60/350], Loss: 0.6473\n",
            "Epoch [61/350], Loss: 0.7782\n",
            "Epoch [62/350], Loss: 0.6044\n",
            "Epoch [63/350], Loss: 0.8059\n",
            "Epoch [64/350], Loss: 0.5408\n",
            "Epoch [65/350], Loss: 0.5992\n",
            "Epoch [66/350], Loss: 0.6146\n",
            "Epoch [67/350], Loss: 0.5777\n",
            "Epoch [68/350], Loss: 0.6925\n",
            "Epoch [69/350], Loss: 0.6402\n",
            "Epoch [70/350], Loss: 0.5997\n",
            "Epoch [71/350], Loss: 0.4216\n",
            "Epoch [72/350], Loss: 0.8948\n",
            "Epoch [73/350], Loss: 0.5733\n",
            "Epoch [74/350], Loss: 0.6726\n",
            "Epoch [75/350], Loss: 0.6742\n",
            "Epoch [76/350], Loss: 0.6435\n",
            "Epoch [77/350], Loss: 0.3811\n",
            "Epoch [78/350], Loss: 0.8155\n",
            "Epoch [79/350], Loss: 0.7952\n",
            "Epoch [80/350], Loss: 0.5861\n",
            "Epoch [81/350], Loss: 0.7883\n",
            "Epoch [82/350], Loss: 0.5678\n",
            "Epoch [83/350], Loss: 0.3575\n",
            "Epoch [84/350], Loss: 0.4677\n",
            "Epoch [85/350], Loss: 0.7147\n",
            "Epoch [86/350], Loss: 0.8376\n",
            "Epoch [87/350], Loss: 0.3518\n",
            "Epoch [88/350], Loss: 0.8052\n",
            "Epoch [89/350], Loss: 0.5339\n",
            "Epoch [90/350], Loss: 0.6471\n",
            "Epoch [91/350], Loss: 0.6795\n",
            "Epoch [92/350], Loss: 0.3866\n",
            "Epoch [93/350], Loss: 0.6507\n",
            "Epoch [94/350], Loss: 0.6015\n",
            "Epoch [95/350], Loss: 0.4354\n",
            "Epoch [96/350], Loss: 0.9881\n",
            "Epoch [97/350], Loss: 0.5621\n",
            "Epoch [98/350], Loss: 0.3146\n",
            "Epoch [99/350], Loss: 0.6765\n",
            "Epoch [100/350], Loss: 1.0556\n",
            "Epoch [101/350], Loss: 0.5724\n",
            "Epoch [102/350], Loss: 0.5344\n",
            "Epoch [103/350], Loss: 0.5521\n",
            "Epoch [104/350], Loss: 0.4827\n",
            "Epoch [105/350], Loss: 0.3957\n",
            "Epoch [106/350], Loss: 0.5247\n",
            "Epoch [107/350], Loss: 0.6839\n",
            "Epoch [108/350], Loss: 0.5712\n",
            "Epoch [109/350], Loss: 0.3390\n",
            "Epoch [110/350], Loss: 0.7476\n",
            "Epoch [111/350], Loss: 0.6141\n",
            "Epoch [112/350], Loss: 0.4846\n",
            "Epoch [113/350], Loss: 0.7159\n",
            "Epoch [114/350], Loss: 0.4268\n",
            "Epoch [115/350], Loss: 0.7087\n",
            "Epoch [116/350], Loss: 0.5220\n",
            "Epoch [117/350], Loss: 0.4072\n",
            "Epoch [118/350], Loss: 0.6506\n",
            "Epoch [119/350], Loss: 0.7148\n",
            "Epoch [120/350], Loss: 1.0709\n",
            "Epoch [121/350], Loss: 0.4407\n",
            "Epoch [122/350], Loss: 0.4334\n",
            "Epoch [123/350], Loss: 0.4229\n",
            "Epoch [124/350], Loss: 0.6008\n",
            "Epoch [125/350], Loss: 0.4315\n",
            "Epoch [126/350], Loss: 0.4115\n",
            "Epoch [127/350], Loss: 0.6758\n",
            "Epoch [128/350], Loss: 1.0543\n",
            "Epoch [129/350], Loss: 0.6466\n",
            "Epoch [130/350], Loss: 0.7333\n",
            "Epoch [131/350], Loss: 1.3044\n",
            "Epoch [132/350], Loss: 0.3136\n",
            "Epoch [133/350], Loss: 0.7356\n",
            "Epoch [134/350], Loss: 0.3371\n",
            "Epoch [135/350], Loss: 0.3594\n",
            "Epoch [136/350], Loss: 0.6685\n",
            "Epoch [137/350], Loss: 0.6282\n",
            "Epoch [138/350], Loss: 0.3851\n",
            "Epoch [139/350], Loss: 0.3133\n",
            "Epoch [140/350], Loss: 0.8438\n",
            "Epoch [141/350], Loss: 0.5630\n",
            "Epoch [142/350], Loss: 0.7465\n",
            "Epoch [143/350], Loss: 0.3134\n",
            "Epoch [144/350], Loss: 0.3179\n",
            "Epoch [145/350], Loss: 0.6018\n",
            "Epoch [146/350], Loss: 0.5679\n",
            "Epoch [147/350], Loss: 0.5462\n",
            "Epoch [148/350], Loss: 0.4965\n",
            "Epoch [149/350], Loss: 0.3297\n",
            "Epoch [150/350], Loss: 0.3605\n",
            "Epoch [151/350], Loss: 0.3446\n",
            "Epoch [152/350], Loss: 0.4001\n",
            "Epoch [153/350], Loss: 0.6942\n",
            "Epoch [154/350], Loss: 0.3192\n",
            "Epoch [155/350], Loss: 0.3544\n",
            "Epoch [156/350], Loss: 0.5044\n",
            "Epoch [157/350], Loss: 0.7070\n",
            "Epoch [158/350], Loss: 0.5342\n",
            "Epoch [159/350], Loss: 0.8616\n",
            "Epoch [160/350], Loss: 0.9572\n",
            "Epoch [161/350], Loss: 1.2029\n",
            "Epoch [162/350], Loss: 0.3258\n",
            "Epoch [163/350], Loss: 0.6298\n",
            "Epoch [164/350], Loss: 0.4035\n",
            "Epoch [165/350], Loss: 0.6539\n",
            "Epoch [166/350], Loss: 0.5871\n",
            "Epoch [167/350], Loss: 0.6206\n",
            "Epoch [168/350], Loss: 0.4979\n",
            "Epoch [169/350], Loss: 0.4499\n",
            "Epoch [170/350], Loss: 0.3361\n",
            "Epoch [171/350], Loss: 0.3324\n",
            "Epoch [172/350], Loss: 1.3050\n",
            "Epoch [173/350], Loss: 0.5582\n",
            "Epoch [174/350], Loss: 0.3133\n",
            "Epoch [175/350], Loss: 0.3133\n",
            "Epoch [176/350], Loss: 0.3133\n",
            "Epoch [177/350], Loss: 0.7710\n",
            "Epoch [178/350], Loss: 0.5201\n",
            "Epoch [179/350], Loss: 0.4920\n",
            "Epoch [180/350], Loss: 0.6768\n",
            "Epoch [181/350], Loss: 0.8103\n",
            "Epoch [182/350], Loss: 0.3333\n",
            "Epoch [183/350], Loss: 0.3196\n",
            "Epoch [184/350], Loss: 0.3982\n",
            "Epoch [185/350], Loss: 0.4401\n",
            "Epoch [186/350], Loss: 0.6003\n",
            "Epoch [187/350], Loss: 1.2356\n",
            "Epoch [188/350], Loss: 0.3133\n",
            "Epoch [189/350], Loss: 0.3290\n",
            "Epoch [190/350], Loss: 0.3138\n",
            "Epoch [191/350], Loss: 0.7870\n",
            "Epoch [192/350], Loss: 0.4215\n",
            "Epoch [193/350], Loss: 0.3577\n",
            "Epoch [194/350], Loss: 0.5960\n",
            "Epoch [195/350], Loss: 0.3509\n",
            "Epoch [196/350], Loss: 0.5333\n",
            "Epoch [197/350], Loss: 1.0424\n",
            "Epoch [198/350], Loss: 0.5684\n",
            "Epoch [199/350], Loss: 0.6906\n",
            "Epoch [200/350], Loss: 0.3164\n",
            "Epoch [201/350], Loss: 0.3505\n",
            "Epoch [202/350], Loss: 1.3110\n",
            "Epoch [203/350], Loss: 0.4547\n",
            "Epoch [204/350], Loss: 0.3233\n",
            "Epoch [205/350], Loss: 0.5612\n",
            "Epoch [206/350], Loss: 0.5792\n",
            "Epoch [207/350], Loss: 0.5072\n",
            "Epoch [208/350], Loss: 0.6881\n",
            "Epoch [209/350], Loss: 0.6015\n",
            "Epoch [210/350], Loss: 0.3290\n",
            "Epoch [211/350], Loss: 1.3112\n",
            "Epoch [212/350], Loss: 0.6389\n",
            "Epoch [213/350], Loss: 0.5205\n",
            "Epoch [214/350], Loss: 0.7590\n",
            "Epoch [215/350], Loss: 0.3219\n",
            "Epoch [216/350], Loss: 0.7920\n",
            "Epoch [217/350], Loss: 0.9561\n",
            "Epoch [218/350], Loss: 0.7278\n",
            "Epoch [219/350], Loss: 0.3774\n",
            "Epoch [220/350], Loss: 0.5844\n",
            "Epoch [221/350], Loss: 0.3165\n",
            "Epoch [222/350], Loss: 0.5360\n",
            "Epoch [223/350], Loss: 0.8381\n",
            "Epoch [224/350], Loss: 0.3162\n",
            "Epoch [225/350], Loss: 0.3184\n",
            "Epoch [226/350], Loss: 1.1864\n",
            "Epoch [227/350], Loss: 0.4062\n",
            "Epoch [228/350], Loss: 0.3247\n",
            "Epoch [229/350], Loss: 0.4557\n",
            "Epoch [230/350], Loss: 0.5774\n",
            "Epoch [231/350], Loss: 0.3365\n",
            "Epoch [232/350], Loss: 0.3133\n",
            "Epoch [233/350], Loss: 0.7283\n",
            "Epoch [234/350], Loss: 0.7598\n",
            "Epoch [235/350], Loss: 0.3134\n",
            "Epoch [236/350], Loss: 0.3323\n",
            "Epoch [237/350], Loss: 0.4247\n",
            "Epoch [238/350], Loss: 0.6301\n",
            "Epoch [239/350], Loss: 1.2789\n",
            "Epoch [240/350], Loss: 0.3603\n",
            "Epoch [241/350], Loss: 0.7649\n",
            "Epoch [242/350], Loss: 0.6801\n",
            "Epoch [243/350], Loss: 0.3133\n",
            "Epoch [244/350], Loss: 0.3141\n",
            "Epoch [245/350], Loss: 0.4363\n",
            "Epoch [246/350], Loss: 0.5506\n",
            "Epoch [247/350], Loss: 0.7737\n",
            "Epoch [248/350], Loss: 0.3618\n",
            "Epoch [249/350], Loss: 0.3753\n",
            "Epoch [250/350], Loss: 0.3133\n",
            "Epoch [251/350], Loss: 0.3445\n",
            "Epoch [252/350], Loss: 0.3267\n",
            "Epoch [253/350], Loss: 0.4360\n",
            "Epoch [254/350], Loss: 0.3133\n",
            "Epoch [255/350], Loss: 0.3779\n",
            "Epoch [256/350], Loss: 0.3864\n",
            "Epoch [257/350], Loss: 1.0905\n",
            "Epoch [258/350], Loss: 0.9419\n",
            "Epoch [259/350], Loss: 0.3998\n",
            "Epoch [260/350], Loss: 0.3133\n",
            "Epoch [261/350], Loss: 0.4983\n",
            "Epoch [262/350], Loss: 0.3226\n",
            "Epoch [263/350], Loss: 0.4706\n",
            "Epoch [264/350], Loss: 0.4153\n",
            "Epoch [265/350], Loss: 0.3446\n",
            "Epoch [266/350], Loss: 0.3898\n",
            "Epoch [267/350], Loss: 0.9823\n",
            "Epoch [268/350], Loss: 0.7274\n",
            "Epoch [269/350], Loss: 0.3211\n",
            "Epoch [270/350], Loss: 0.3143\n",
            "Epoch [271/350], Loss: 0.3669\n",
            "Epoch [272/350], Loss: 0.3133\n",
            "Epoch [273/350], Loss: 0.4395\n",
            "Epoch [274/350], Loss: 0.3169\n",
            "Epoch [275/350], Loss: 0.3133\n",
            "Epoch [276/350], Loss: 0.5266\n",
            "Epoch [277/350], Loss: 0.4098\n",
            "Epoch [278/350], Loss: 0.5044\n",
            "Epoch [279/350], Loss: 0.4021\n",
            "Epoch [280/350], Loss: 0.9179\n",
            "Epoch [281/350], Loss: 1.2509\n",
            "Epoch [282/350], Loss: 0.4025\n",
            "Epoch [283/350], Loss: 0.5730\n",
            "Epoch [284/350], Loss: 0.3664\n",
            "Epoch [285/350], Loss: 0.3256\n",
            "Epoch [286/350], Loss: 0.5231\n",
            "Epoch [287/350], Loss: 0.3803\n",
            "Epoch [288/350], Loss: 0.6657\n",
            "Epoch [289/350], Loss: 0.8020\n",
            "Epoch [290/350], Loss: 0.3133\n",
            "Epoch [291/350], Loss: 0.3133\n",
            "Epoch [292/350], Loss: 0.3136\n",
            "Epoch [293/350], Loss: 0.7795\n",
            "Epoch [294/350], Loss: 1.1334\n",
            "Epoch [295/350], Loss: 0.3261\n",
            "Epoch [296/350], Loss: 0.3163\n",
            "Epoch [297/350], Loss: 0.6218\n",
            "Epoch [298/350], Loss: 0.5615\n",
            "Epoch [299/350], Loss: 0.3221\n",
            "Epoch [300/350], Loss: 0.4092\n",
            "Epoch [301/350], Loss: 0.3161\n",
            "Epoch [302/350], Loss: 0.3133\n",
            "Epoch [303/350], Loss: 0.3134\n",
            "Epoch [304/350], Loss: 1.3072\n",
            "Epoch [305/350], Loss: 0.5569\n",
            "Epoch [306/350], Loss: 0.3134\n",
            "Epoch [307/350], Loss: 0.6101\n",
            "Epoch [308/350], Loss: 0.7383\n",
            "Epoch [309/350], Loss: 0.3134\n",
            "Epoch [310/350], Loss: 0.4724\n",
            "Epoch [311/350], Loss: 0.3134\n",
            "Epoch [312/350], Loss: 0.6131\n",
            "Epoch [313/350], Loss: 0.8852\n",
            "Epoch [314/350], Loss: 0.3133\n",
            "Epoch [315/350], Loss: 0.3478\n",
            "Epoch [316/350], Loss: 0.8086\n",
            "Epoch [317/350], Loss: 0.4263\n",
            "Epoch [318/350], Loss: 0.3723\n",
            "Epoch [319/350], Loss: 0.3133\n",
            "Epoch [320/350], Loss: 0.3136\n",
            "Epoch [321/350], Loss: 0.9412\n",
            "Epoch [322/350], Loss: 0.5906\n",
            "Epoch [323/350], Loss: 1.1446\n",
            "Epoch [324/350], Loss: 0.8535\n",
            "Epoch [325/350], Loss: 0.5868\n",
            "Epoch [326/350], Loss: 0.6019\n",
            "Epoch [327/350], Loss: 1.0455\n",
            "Epoch [328/350], Loss: 0.3274\n",
            "Epoch [329/350], Loss: 0.4837\n",
            "Epoch [330/350], Loss: 0.7966\n",
            "Epoch [331/350], Loss: 1.3133\n",
            "Epoch [332/350], Loss: 0.5645\n",
            "Epoch [333/350], Loss: 0.3379\n",
            "Epoch [334/350], Loss: 0.3301\n",
            "Epoch [335/350], Loss: 0.3133\n",
            "Epoch [336/350], Loss: 0.3324\n",
            "Epoch [337/350], Loss: 0.4694\n",
            "Epoch [338/350], Loss: 0.5380\n",
            "Epoch [339/350], Loss: 0.6515\n",
            "Epoch [340/350], Loss: 0.4986\n",
            "Epoch [341/350], Loss: 0.4691\n",
            "Epoch [342/350], Loss: 0.3350\n",
            "Epoch [343/350], Loss: 0.3828\n",
            "Epoch [344/350], Loss: 0.9602\n",
            "Epoch [345/350], Loss: 0.4694\n",
            "Epoch [346/350], Loss: 1.3055\n",
            "Epoch [347/350], Loss: 0.4058\n",
            "Epoch [348/350], Loss: 0.4112\n",
            "Epoch [349/350], Loss: 0.6279\n",
            "Epoch [350/350], Loss: 0.3134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, x_test, y_test):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x_test)  # Get predictions\n",
        "        _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "\n",
        "        _, labels = torch.max(y_test_tensor, 1)  # Convert one-hot to single label\n",
        "\n",
        "        accuracy = (predicted == labels).float().mean()  # Calculate accuracy\n",
        "        return accuracy.item()\n",
        "\n",
        "# Check accuracy on the test set\n",
        "accuracy = evaluate_model(model, x_test_tensor, y_test_tensor)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHFHnwmYoYUk",
        "outputId": "46af8a37-b5a2-4f51-8fe4-aab4caa8f0af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 71.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(model, x_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x_test)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        _, true_labels = torch.max(y_test, 1)\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(true_labels.numpy(), predicted.numpy())\n",
        "\n",
        "        # Plot the confusion matrix using seaborn\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.show()\n",
        "\n",
        "# Call the function to plot confusion matrix\n",
        "plot_confusion_matrix(model, x_test_tensor, y_test_tensor)\n",
        "\n",
        "\n",
        "#The outputs of True Positive and True Negatives are much higher than others"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "5oLax3eCiHdj",
        "outputId": "5551f69d-99cc-452f-94b0-8a5eb85b8ad9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_classification_metrics(model, x_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x_test)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        _, true_labels = torch.max(y_test, 1)\n",
        "\n",
        "        # Calculate precision, recall, and F1-score\n",
        "        precision = precision_score(true_labels.numpy(), predicted.numpy())\n",
        "        recall = recall_score(true_labels.numpy(), predicted.numpy())\n",
        "        f1 = f1_score(true_labels.numpy(), predicted.numpy())\n",
        "\n",
        "        return precision, recall, f1\n",
        "\n",
        "# Get classification metrics\n",
        "precision, recall, f1 = evaluate_classification_metrics(model, x_test_tensor, y_test_tensor)\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYCgXHokmT_l",
        "outputId": "3ee39ba0-4f4f-498f-b9a0-2f54d8348de4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7228\n",
            "Recall: 0.7878\n",
            "F1 Score: 0.7539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_on_new_input(model, custom_input):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        custom_input_tensor = torch.FloatTensor(custom_input).unsqueeze(0)  # Ensure it's a batch (1 sample)\n",
        "        output = model(custom_input_tensor)  # Get model output\n",
        "        _, predicted = torch.max(output, 1)  # Get class with highest probability\n",
        "\n",
        "        return predicted.item()  # Return the predicted class index\n",
        "\n",
        "# Example custom input: scaled values for Open, Close, High, Low, MA10, RSI (assuming scaling is done the same way as training data)\n",
        "custom_input = [0.5, 0.3, 0.7, 0.6, 0.8, 0.4]  # Example custom data (normalized/scaled)\n",
        "prediction = test_on_new_input(model, custom_input)\n",
        "\n",
        "print(f'Predicted class for custom input: {\"Up\" if prediction == 1 else \"Down\"}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwA95Cj1moY2",
        "outputId": "7e14cb61-94d9-4f6e-f4fd-2d52e9bfcb76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class for custom input: Down\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3YCxMhCyrQI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Assuming the model and scaler are already trained and fitted on training data\n",
        "# Example for the model and scaler (in case it's not already done)\n",
        "# For this example, we assume 'scaler' was fitted on the training data (as in previous examples)\n",
        "\n",
        "# Assuming you have a trained model called 'model' and a fitted scaler called 'scaler'\n",
        "# Also assuming you have 'input_size' and 'output_size' already defined\n",
        "\n",
        "# Let's define a function that normalizes new real-world data\n",
        "\n",
        "def normalize_and_predict(raw_data, model, scaler):\n",
        "    \"\"\"\n",
        "    Takes raw real-world input data, normalizes it using the training data's scaler,\n",
        "    and then inputs it into the trained model to predict the outcome.\n",
        "\n",
        "    Parameters:\n",
        "    - raw_data: pd.DataFrame, raw real-world data (e.g., stock data) to be normalized.\n",
        "    - model: The trained PyTorch neural network model.\n",
        "    - scaler: The StandardScaler fitted on training data to normalize the data.\n",
        "\n",
        "    Returns:\n",
        "    - predictions: The model's predictions for the input data.\n",
        "    \"\"\"\n",
        "    # Step 1: Normalize the new raw data using the fitted scaler (from training)\n",
        "    # Ensure that the raw data columns match the columns used during training\n",
        "    raw_data_scaled = scaler.transform(raw_data)\n",
        "\n",
        "    # Step 2: Convert the scaled data to PyTorch tensor\n",
        "    raw_data_tensor = torch.FloatTensor(raw_data_scaled)\n",
        "\n",
        "    # Step 3: Use the trained model to predict the outcome\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # No need to calculate gradients during prediction\n",
        "        predictions = model(raw_data_tensor)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Step 1: Define the real-world data you want to test\n",
        "# This should have the same feature columns as the training data\n",
        "new_data = pd.DataFrame({\n",
        "    'Open': [223.58],  # Real-world raw data for 'Open' prices\n",
        "    'Close/Last': [222.50],\n",
        "    'High': [224.04],\n",
        "    'Low': [221.91],\n",
        "    'MA10': [222.50],\n",
        "    'RSI': [55.1]\n",
        "})\n",
        "\n",
        "# Step 2: Normalize and get predictions\n",
        "predictions = normalize_and_predict(new_data, model, scaler)\n",
        "\n",
        "# Step 3: Print out the predictions\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhpagTxRnh5T",
        "outputId": "31d6b800-838d-4938-c029-6a824d223663"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "tensor([[0.9734, 0.0266]])\n"
          ]
        }
      ]
    }
  ]
}